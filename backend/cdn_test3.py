import asyncio
import csv
import re
import time
import urllib.parse
from datetime import datetime
from aiohttp import ClientSession, TCPConnector

# é…ç½®é¡¹
VIDEO_URLS = {
    "cdn": "https://lancet.im/videos/hls/playlist_local.m3u8",
    "r2": "https://pub-8ea55317b8624238a35e5c73454b9d2d.r2.dev/videos/hls/playlist_local.m3u8",
}
proxy = "http://127.0.0.1:1080"

CONCURRENT_USERS = 10
TEST_DURATION = 60          # æ‹‰æµæ—¶é•¿ï¼Œç§’
NUM_ROUNDS = 1
INTERVAL_MINUTES = 30
MAX_TS_CHECK = 13            # é‡‡æ ·å‰å‡ ä¸ª ts

# è¯·æ±‚å•ä¸ª ts çš„ HEADï¼Œè¿”å›çŠ¶æ€ç å’Œå…³é”®å¤´éƒ¨
async def fetch_head_info(url, session):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36",
        "Accept-Encoding": "gzip, deflate, br",
        "Accept": "*/*"
    }
    try:
        async with session.get(url, headers=headers) as resp:
            headers = {k.lower(): v for k, v in resp.headers.items()}
            return {
                "status": resp.status,
                "cf_cache_status": headers.get("cf-cache-status", "N/A"),
                "cf_ray": headers.get("cf-ray", "N/A"),
                "url": url
            }
    except Exception as e:
        return {"status": 0, "cf_cache_status": "ERROR", "cf_ray": "ERROR", "url": url}

# è§£æ m3u8 è·å– TS ç‰‡æ®µå®Œæ•´ URL åˆ—è¡¨
async def get_ts_urls(m3u8_url, session, max_count=MAX_TS_CHECK):
    ts_urls = []
    try:
        async with session.get(m3u8_url) as resp:
            if resp.status != 200:
                print(f"Warning: m3u8 è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç  {resp.status}")
                return ts_urls
            text = await resp.text()
            for line in text.splitlines():
                line = line.strip()
                if line and not line.startswith("#") and line.endswith(".ts"):
                    ts_url = urllib.parse.urljoin(m3u8_url, line)
                    ts_urls.append(ts_url)
                    if len(ts_urls) >= max_count:
                        break
    except Exception as e:
        print(f"Exception in get_ts_urls: {e}")
    return ts_urls

# ä¸»æµ‹è¯•é€»è¾‘ï¼Œæ‹‰æµ + é‡‡é›†å¤šä¸ª ts ç‰‡æ®µå¤´éƒ¨ä¿¡æ¯ + è®¡ç®—é¦–å±æ—¶é—´
async def run_ffmpeg_test(index, url, session):
    # 1. å…ˆè¯·æ±‚ M3U8 è®¡æ—¶
    t0 = time.time()
    try:
        async with session.get(url) as resp:
            if resp.status != 200:
                print(f"User {index} è­¦å‘Šï¼šM3U8 è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç  {resp.status}")
            m3u8_text = await resp.text()
    except Exception as e:
        print(f"User {index} M3U8 è¯·æ±‚å¼‚å¸¸: {e}")
        m3u8_text = ""

    m3u8_time = time.time() - t0

    # 2. è§£æ TS åˆ—è¡¨ï¼ˆé™åˆ¶å‰ MAX_TS_CHECK ä¸ªï¼‰
    ts_urls = []
    for line in m3u8_text.splitlines():
        line = line.strip()
        if line and not line.startswith("#") and line.endswith(".ts"):
            ts_url = urllib.parse.urljoin(url, line)
            ts_urls.append(ts_url)
            if len(ts_urls) >= MAX_TS_CHECK:
                break

    # 3. è¯·æ±‚ç¬¬ä¸€ä¸ª TS è®¡æ—¶ï¼Œä½œä¸ºé¦–å±æ—¶é—´çš„ä¸€éƒ¨åˆ†
    first_ts_time = None
    if ts_urls:
        t_start = time.time()
        first_ts_info = await fetch_head_info(ts_urls[0], session)
        first_ts_time = time.time() - t_start
    else:
        first_ts_info = None
        first_ts_time = None

    # 4. å¹¶å‘è¯·æ±‚æ‰€æœ‰ TS å¤´éƒ¨
    tasks = [fetch_head_info(ts_url, session) for ts_url in ts_urls]
    ts_results = await asyncio.gather(*tasks)

    # 5. å¯åŠ¨ ffmpeg æ‹‰æµï¼Œæ”¶é›† fpsã€speedã€frameã€dropped

    cmd = [
        "ffmpeg",
        "-loglevel", "info",  # æ”¹æˆ info çº§åˆ«ï¼Œä¿è¯è¾“å‡ºçŠ¶æ€ä¿¡æ¯
        #"-http_proxy", proxy,
        "-i", url,
        "-t", str(TEST_DURATION),
        "-f", "null", "-"
    ]

    start_time = time.time()
    proc = await asyncio.create_subprocess_exec(
        *cmd,
        stdout=asyncio.subprocess.DEVNULL,
        stderr=asyncio.subprocess.PIPE
    )
    _, stderr = await proc.communicate()
    duration = time.time() - start_time

    stderr_decoded = stderr.decode()
    speed_match = re.findall(r"speed=\s*([\d\.]+)x", stderr_decoded)
    fps_match = re.findall(r"fps=\s*(\d+)", stderr_decoded)
    frame_match = re.findall(r"frame=\s*(\d+)", stderr_decoded)
    drop_match = re.findall(r"drop.*?=\s*(\d+)", stderr_decoded)

    # 6. ç»Ÿè®¡æ•°æ®
    ts_statuses = [r["status"] for r in ts_results]
    ts_cache_hits = sum(1 for r in ts_results if r["cf_cache_status"] == "HIT")
    ts_404_count = sum(1 for s in ts_statuses if s == 404)

    # 7. è®¡ç®—é¦–å±æ—¶é—´ = M3U8è¯·æ±‚æ—¶é—´ + ç¬¬ä¸€ä¸ªTSè¯·æ±‚æ—¶é—´
    first_screen_time = None
    if m3u8_time is not None and first_ts_time is not None:
        first_screen_time = round(m3u8_time + first_ts_time, 3)

    return {
        "user": index,
        "start_time": datetime.now().isoformat(),
        "duration_s": round(duration, 2),
        "speed": float(speed_match[-1]) if speed_match else None,
        "fps": int(fps_match[-1]) if fps_match else None,
        "frames": int(frame_match[-1]) if frame_match else None,
        "dropped": int(drop_match[-1]) if drop_match else 0,
        "return_code": proc.returncode,
        "ts_statuses": ts_statuses,
        "ts_cache_hits": ts_cache_hits,
        "ts_404_count": ts_404_count,
        "first_screen_time": first_screen_time,
        "ts_details": ts_results,
    }

async def run_round(round_num, label, url):
    print(f"\nğŸš€ Round {round_num} - Testing [{label}]")
    connector = TCPConnector(limit=None)
    async with ClientSession(connector=connector) as session:
        tasks = [run_ffmpeg_test(i + 1, url, session) for i in range(CONCURRENT_USERS)]
        results = await asyncio.gather(*tasks)

    # ç»Ÿè®¡åˆ†æ
    valid_first_screen_times = [r["first_screen_time"] for r in results if r["first_screen_time"] is not None]
    avg_first_screen = sum(valid_first_screen_times) / len(valid_first_screen_times) if valid_first_screen_times else None
    total_cache_hits = sum(r["ts_cache_hits"] for r in results)
    total_ts = sum(len(r["ts_statuses"]) for r in results)
    total_404 = sum(r["ts_404_count"] for r in results)
    cache_hit_rate = total_cache_hits / total_ts if total_ts > 0 else None
    error_rate = total_404 / total_ts if total_ts > 0 else None

    print(f"â¡ï¸ å¹³å‡é¦–å±æ—¶é—´: {avg_first_screen:.3f} ç§’" if avg_first_screen else "æ— æœ‰æ•ˆé¦–å±æ—¶é—´æ•°æ®")
    print(f"â¡ï¸ ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.2%}" if cache_hit_rate is not None else "æ— ç¼“å­˜å‘½ä¸­æ•°æ®")
    print(f"â¡ï¸ 404 é”™è¯¯ç‡: {error_rate:.2%}" if error_rate is not None else "æ— é”™è¯¯ç‡æ•°æ®")

    # è¾“å‡º CSV
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"{label}_round{round_num}_{timestamp}.csv"
    with open(output_file, "w", newline="", encoding="utf-8") as f:
        fieldnames = list(results[0].keys())
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for r in results:
            r["ts_details"] = str(r["ts_details"])
            writer.writerow(r)
    print(f"âœ… ç»“æœå·²ä¿å­˜è‡³: {output_file}")

async def run_all_rounds():
    for round_num in range(1, NUM_ROUNDS + 1):
        for label, url in VIDEO_URLS.items():
            await run_round(round_num, label, url)
        if round_num < NUM_ROUNDS:
            print(f"â³ ç­‰å¾… {INTERVAL_MINUTES} åˆ†é’Ÿè¿›è¡Œä¸‹ä¸€è½®æµ‹è¯•...\n")
            await asyncio.sleep(INTERVAL_MINUTES * 60)

if __name__ == "__main__":
    import warnings
    warnings.filterwarnings("ignore", category=RuntimeWarning)

    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        loop.run_until_complete(run_all_rounds())
    finally:
        loop.run_until_complete(asyncio.sleep(1))
        loop.close()
