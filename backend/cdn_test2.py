import asyncio
import csv
import re
import time
import urllib.parse
from datetime import datetime
from aiohttp import ClientSession, TCPConnector

# é…ç½®é¡¹
VIDEO_URLS = {
    "cdn": "https://lancet.im/videos/hls/playlist_local.m3u8",
    "r2": "https://pub-8ea55317b8624238a35e5c73454b9d2d.r2.dev/videos/hls/playlist_local.m3u8",
}
proxy = "http://127.0.0.1:1082"

CONCURRENT_USERS = 10
TEST_DURATION = 15           # æ‹‰æµæ—¶é•¿ï¼Œç§’
NUM_ROUNDS = 1
INTERVAL_MINUTES = 30
MAX_TS_CHECK = 5            # é‡‡æ ·å‰å‡ ä¸ª ts

# è¯·æ±‚å•ä¸ª ts çš„ HEADï¼Œè¿”å›çŠ¶æ€ç å’Œå…³é”®å¤´éƒ¨
async def fetch_head_info(url, session):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36",
        "Accept-Encoding": "gzip, deflate, br",
        "Accept": "*/*"
    }
    try:
        async with session.get(url, headers=headers, proxy=proxy) as resp:
            # ä¸è¯»å–å†…å®¹ï¼Œåªå– header
            headers = {k.lower(): v for k, v in resp.headers.items()}
            return {
                "status": resp.status,
                "cf_cache_status": headers.get("cf-cache-status", "N/A"),
                "cf_ray": headers.get("cf-ray", "N/A"),
                "url": url
            }
    except Exception as e:
        return {"status": 0, "cf_cache_status": "ERROR", "cf_ray": "ERROR", "url": url}

# è§£æ m3u8 è·å– TS ç‰‡æ®µå®Œæ•´ URL åˆ—è¡¨
async def get_ts_urls(m3u8_url, session, max_count=MAX_TS_CHECK):
    ts_urls = []
    try:
        #print("1")
        async with session.get(m3u8_url, proxy=proxy) as resp:
            #print("2")
            if resp.status != 200:
                print(f"Warning: m3u8 è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç  {resp.status}")
                return ts_urls
            text = await resp.text()
            for line in text.splitlines():
                line = line.strip()
                if line and not line.startswith("#") and line.endswith(".ts"):
                    # æ‹¼æ¥ç»å¯¹URL
                    ts_url = urllib.parse.urljoin(m3u8_url, line)
                    #print("ts_url",ts_url)
                    ts_urls.append(ts_url)
                    if len(ts_urls) >= max_count:
                        #ts_urls = ["https://lancet.im/videos/hls/ts/880955976765-3-602540_2006_4_d0_mb220207_sb220207.ts"]
                        break
    except Exception as e:
        print(f"Exception in get_ts_urls: {e}")
    return ts_urls

# ä¸»æµ‹è¯•é€»è¾‘ï¼Œæ‹‰æµ + é‡‡é›†å¤šä¸ª ts ç‰‡æ®µå¤´éƒ¨ä¿¡æ¯
async def run_ffmpeg_test(index, url, session):
    cmd = [
        "ffmpeg",
        "-loglevel", "debug",  # ğŸ‘ˆ è¾“å‡ºè¯¦ç»†ä¿¡æ¯ç”¨äºè°ƒè¯•
        "-http_proxy", proxy,  # ğŸ‘ˆ å¼ºåˆ¶èµ°ä»£ç†
        "-i", url,
        "-t", str(TEST_DURATION),
        "-f", "null", "-"
    ]

    start_time = time.time()
    proc = await asyncio.create_subprocess_exec(
        *cmd,
        stdout=asyncio.subprocess.DEVNULL,
        stderr=asyncio.subprocess.PIPE
    )
    _, stderr = await proc.communicate()
    duration = time.time() - start_time

    stderr_decoded = stderr.decode()
    speed_match = re.findall(r"speed=\s*([\d\.]+)x", stderr_decoded)
    fps_match = re.findall(r"fps=\s*(\d+)", stderr_decoded)
    frame_match = re.findall(r"frame=\s*(\d+)", stderr_decoded)
    drop_match = re.findall(r"drop.*?=\s*(\d+)", stderr_decoded)

    # è§£æå¤šä¸ªTS
    ts_urls = await get_ts_urls(url, session)
    ts_results = []
    for ts_url in ts_urls:
        start_ts = time.time()
        head_info = await fetch_head_info(ts_url, session)
        head_info["elapsed"] = round(time.time() - start_ts, 3)
        ts_results.append(head_info)

    # ç®€å•ç»Ÿè®¡
    ts_statuses = [r["status"] for r in ts_results]
    ts_cache_hits = sum(1 for r in ts_results if r["cf_cache_status"] == "HIT")
    ts_404_count = sum(1 for s in ts_statuses if s == 404)

    return {
        "user": index,
        "start_time": datetime.now().isoformat(),
        "duration_s": round(duration, 2),
        "speed": float(speed_match[-1]) if speed_match else None,
        "fps": int(fps_match[-1]) if fps_match else None,
        "frames": int(frame_match[-1]) if frame_match else None,
        "dropped": int(drop_match[-1]) if drop_match else 0,
        "return_code": proc.returncode,
        "ts_statuses": ts_statuses,
        "ts_cache_hits": ts_cache_hits,
        "ts_404_count": ts_404_count,
        "ts_details": ts_results,
    }

async def run_round(round_num, label, url):
    print(f"\nğŸš€ Round {round_num} - Testing [{label}]")
    connector = TCPConnector(limit=None)
    async with ClientSession(connector=connector) as session:
        tasks = [run_ffmpeg_test(i + 1, url, session) for i in range(CONCURRENT_USERS)]
        results = await asyncio.gather(*tasks)

    # è¾“å‡º CSV
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"{label}_round{round_num}_{timestamp}.csv"
    with open(output_file, "w", newline="", encoding="utf-8") as f:
        # æ‰©å±•å­—æ®µ ts_details åºåˆ—åŒ–ä¸ºå­—ç¬¦ä¸²
        fieldnames = list(results[0].keys())
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for r in results:
            r["ts_details"] = str(r["ts_details"])
            writer.writerow(r)
    print(f"âœ… ç»“æœå·²ä¿å­˜è‡³: {output_file}")

async def run_all_rounds():
    for round_num in range(1, NUM_ROUNDS + 1):
        for label, url in VIDEO_URLS.items():
            await run_round(round_num, label, url)
        if round_num < NUM_ROUNDS:
            print(f"â³ ç­‰å¾… {INTERVAL_MINUTES} åˆ†é’Ÿè¿›è¡Œä¸‹ä¸€è½®æµ‹è¯•...\n")
            await asyncio.sleep(INTERVAL_MINUTES * 60)

if __name__ == "__main__":
        import warnings

        warnings.filterwarnings("ignore", category=RuntimeWarning)

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(run_all_rounds())
        finally:
            loop.run_until_complete(asyncio.sleep(1))  # è®© __del__ æœ‰æœºä¼šè·‘å®Œ
            loop.close()

